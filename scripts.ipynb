{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84ed5944",
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd /content/mdlARC/\n",
    "from pathlib import Path\n",
    "import argparse\n",
    "import importlib\n",
    "import utils, train\n",
    "\n",
    "importlib.reload(utils)  # pick up code changes during iteration\n",
    "importlib.reload(train)\n",
    "\n",
    "args = {\n",
    "    # run config\n",
    "    \"num_workers\": 0,\n",
    "    \"device\": \"cuda\",  # 'cuda' | 'mps' | 'cpu'\n",
    "    # paths - must pass as Path(\"<path_to_dir>\")\n",
    "    \"save_path\": Path(\"runs/tiny.pt\"),\n",
    "    \"checkpoint_path\": None,  # Path(\"runs/tiny.pt\"),  # or None to start from scratch\n",
    "    # \"data_path\": Path(\"assets/script-tests/grouped-tasks-00d62c1b/challenges.json\" ),\n",
    "    \"data_path\": Path(\"assets/script-tests/grouped-tasks/challenges.json\"),\n",
    "    # \"data_path\": Path(\"assets/ARC-1/grouped-tasks/training/challenges.json\"),\n",
    "    # \"data_path\": Path(\"assets/ARC-2/grouped-tasks/training/challenges.json\"),\n",
    "    # hyperparameters\n",
    "    \"epochs\": 5,\n",
    "    \"batch_size\": 110,\n",
    "    \"lr\": 3e-4,\n",
    "    \"weight_decay\": 0.01,\n",
    "    \"grad_clip\": 1.0,\n",
    "    \"seed\": 42,\n",
    "    # Visibility toggles\n",
    "    \"log_train_strings\": False,\n",
    "    \"log_train_limit\": 10,\n",
    "    \"log_inference_prompt\": False,\n",
    "}\n",
    "cfg = argparse.Namespace(**args)\n",
    "model, dataset, dataloader, device, data_path = train.build_model_and_data(cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e6ac355",
   "metadata": {
    "tags": [
     "train"
    ]
   },
   "outputs": [],
   "source": [
    "# Training only\n",
    "train.train_model(\n",
    "    cfg,\n",
    "    model=model,\n",
    "    dataloader=dataloader,\n",
    "    dataset=dataset,\n",
    "    device=device,\n",
    "    data_path=data_path,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a215afbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import inference\n",
    "from utils import plot_grids, split_grids_from_tokens, tokens_to_string\n",
    "\n",
    "importlib.reload(utils)\n",
    "importlib.reload(inference)\n",
    "\n",
    "task_ids_list = [\"00d62c1b\", \"e0fb7511\", \"00576224\", \"3aa6fb7a\"]  # always pass as list\n",
    "selected_split = \"test\"\n",
    "# selected_split = \"train\"\n",
    "pair_idx = 0\n",
    "visualise = True\n",
    "\n",
    "results = inference.run_batched_inference(\n",
    "    model=model,\n",
    "    dataset=dataset,\n",
    "    task_ids=task_ids_list,\n",
    "    device=device,\n",
    "    split=selected_split,\n",
    "    pair_index=pair_idx,\n",
    "    include_targets=True,\n",
    ")\n",
    "\n",
    "if not results:\n",
    "    print(\"No inference results were produced.\")\n",
    "for res in results:\n",
    "    print(f\"\\nTask {res['task_id']} pair {res['pair_index']} ({selected_split})\")\n",
    "    print(\"Prompt tokens:\", tokens_to_string(res[\"prompt_tokens\"]))\n",
    "    print(\"Generated output tokens:\", tokens_to_string(res[\"output_tokens\"]))\n",
    "    if res.get(\"target_output_tokens\"):\n",
    "        print(\"Target output tokens:\", tokens_to_string(res[\"target_output_tokens\"]))\n",
    "    print(\"Predicted grid:\")\n",
    "    for row in res[\"output_grid\"]:\n",
    "        print(row)\n",
    "    if res.get(\"target_grid\"):\n",
    "        print(\"Target grid:\")\n",
    "        for row in res[\"target_grid\"]:\n",
    "            print(row)\n",
    "    if visualise:\n",
    "        prompt_grids = split_grids_from_tokens(res[\"prompt_tokens\"])\n",
    "        input_grid = prompt_grids[0] if prompt_grids else []\n",
    "        to_plot = [input_grid, res[\"output_grid\"]]\n",
    "        if res.get(\"target_grid\"):\n",
    "            to_plot.append(res[\"target_grid\"])\n",
    "        plot_grids(\n",
    "            to_plot,\n",
    "            title=f\"{res['task_id']} pair {res['pair_index']} ({selected_split})\",\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "single-inference",
   "metadata": {},
   "outputs": [],
   "source": [
    "import inference\n",
    "\n",
    "importlib.reload(inference)\n",
    "\n",
    "EVAL_BATCH_SIZE = 1300\n",
    "\n",
    "evaluation = inference.evaluate_model_on_dataset(\n",
    "    model=model,\n",
    "    dataset=dataset,\n",
    "    device=device,\n",
    "    batch_size=EVAL_BATCH_SIZE,\n",
    "    log_prompts=args[\"log_inference_prompt\"],\n",
    ")\n",
    "\n",
    "for split in (\"train\", \"test\"):\n",
    "    summary = evaluation.get(split, {}).get(\"summary\", {})\n",
    "    total = summary.get(\"total_sequences\", 0)\n",
    "    shape_ok = summary.get(\"num_shape_correct\", 0)\n",
    "    avg_pixel_acc = summary.get(\"avg_pixel_accuracy\", 0.0)\n",
    "    fully_correct = summary.get(\"num_fully_correct\", 0)\n",
    "\n",
    "    print(f\"\\nSplit: {split}\")\n",
    "    print(f\"  sequences evaluated: {total}\")\n",
    "    print(f\"  correct output grid shapes: {shape_ok} / {total}\")\n",
    "    if shape_ok > 0:\n",
    "        print(f\"  avg pixel accuracy (shape-correct only): {avg_pixel_acc:.4f}\")\n",
    "    else:\n",
    "        print(\"  avg pixel accuracy (shape-correct only): n/a\")\n",
    "    print(f\"  fully correct output grids: {fully_correct} / {total}\")\n",
    "\n",
    "    if split == \"test\":\n",
    "        correct_outputs = summary.get(\"fully_correct_results\", [])\n",
    "        print(\"  fully correct test outputs (task_id, pair_index, grid):\")\n",
    "        if not correct_outputs:\n",
    "            print(\"    (none)\")\n",
    "        for res in correct_outputs:\n",
    "            grid = res.get(\"output_grid\", [])\n",
    "            print(f\"    - {res.get('task_id')} pair {res.get('pair_index')}: {grid}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
